{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h0Vh7tlM_VK9",
        "VNHAAJuCZCqE",
        "McGKfLtWnsTB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jki5410/DE-3rd/blob/main/Spark/PySpark_%EC%84%A4%EC%B9%98_%EB%B0%8F_%ED%85%8C%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqhTDfuWrcM"
      },
      "source": [
        "PySpark을 로컬머신에 설치하고 노트북을 사용하기 보다는 머신러닝 관련 다양한 라이브러리가 이미 설치되었고 좋은 하드웨어를 제공해주는 Google Colab을 통해 실습을 진행한다. 이를 위해 pyspark과 Py4J 패키지를 설치한다.\n",
        "\n",
        "PySpark을 설치하면 Local Standalone Spark이 설치되고 이 Spark 클러스터를 파이썬에서 액세스해야되기 때문에 Py4J 설치가 필요하다.\n",
        "Py4J 패키지는 파이썬 프로그램이 자바가상머신상의 오브젝트들을 접근할 수 있게 해준다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark 설치"
      ],
      "metadata": {
        "id": "h0Vh7tlM_VK9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbT0rpGfVdiq",
        "outputId": "35903e3b-ad56-4755-abd8-5eea84df6040"
      },
      "source": [
        "!pip install pyspark==3.3.1 py4j==0.10.9.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.1\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845493 sha256=fc6743773ea7f5aaa212a1ed27ca0ae43fc9e848bf63adce7765d96d3c21a5ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/f0/3d/517368b8ce80486e84f89f214e0a022554e4ee64969f46279b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_eTGrvXlDw"
      },
      "source": [
        "**Spark Session:** SparkSession은 Spark 2.0부터 엔트리 포인트로 사용된다. SparkSession을 이용해 RDD, 데이터 프레임등을 만든다. SparkSession은 SparkSession.builder를 호출하여 생성하며 다양한 함수들을 통해 세부 설정이 가능하다\n",
        "\n",
        "* local[*] Spark이 하나의 JVM으로 동작하고 그 안에 컴퓨터의 코어 수 만큼의 스레드가 Executor로 동작한다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vm6tgcPXdnR"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark Tutorial')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSs_1PYaYWxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "bc5a409a-fdc7-4f07-df05-59a1722b9ac7"
      },
      "source": [
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b1a0c0f1f00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6b03513774b2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "oeEFwKoZSVaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddd0e61-2c6c-43f0-c922-3538ced77337"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:           6\n",
            "    Model:                79\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             0\n",
            "    BogoMIPS:             4400.36\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
            "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
            "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
            "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
            "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
            "                          fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx sm\n",
            "                          ap xsaveopt arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     256 KiB (1 instance)\n",
            "  L3:                     55 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
            "                          pgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BH\n",
            "                          I: Vulnerable (Syscall hardening enabled)\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cpu가 두 개!\n",
        "즉, 두 개의 쓰레드가 돌 수 있다."
      ],
      "metadata": {
        "id": "i9GVtQCqeCwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep MemTotal /proc/meminfo"
      ],
      "metadata": {
        "id": "eIF5oQkLSo4-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd882f9-57ea-47a3-da0a-6c54da2728b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13290464 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python <> RDD <> DataFrame"
      ],
      "metadata": {
        "id": "f8gMf2J1VLqn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNHAAJuCZCqE"
      },
      "source": [
        "### **Python 객체를 RDD로 변환해보기**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1> Python 리스트 생성**"
      ],
      "metadata": {
        "id": "IC6kHYiNnocR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhijTd1UY1i9"
      },
      "source": [
        "name_list_json = [ '{\"name\": \"keeyong\"}', '{\"name\": \"benjamin\"}', '{\"name\": \"claire\"}' ]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "json처럼 보이지만, 작은 따옴표로 구성되어있는 string 인스턴스로 나열된 리스트이다"
      ],
      "metadata": {
        "id": "xswLLCiwgXce"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXqf1lC_Zdxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978b79b9-9c46-4b23-cd4e-889779185c0a"
      },
      "source": [
        "for n in name_list_json:\n",
        "  print(n)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"keeyong\"}\n",
            "{\"name\": \"benjamin\"}\n",
            "{\"name\": \"claire\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlanTznb75N"
      },
      "source": [
        "**2> 파이썬 리스트를 RDD로 변환**\n",
        "\n",
        " * RDD로 변환되는 순간 Spark 클러스터의 서버들에 데이터가 나눠 저장됨 (파티션)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 spark session object를 만들어 spark이라는 변수에 저장했던 해당 spark변수를 활용하여 RDD로 변환하는 과정!\n",
        "\n",
        "sparkContext를 이용해 RDD와 관련된 작업들을 할 수 있다.\n",
        "\n",
        "parallelize함수를 부르며 위의 스트링으로 구성된 파이썬 리스트를 넘기면 RDD가 만들어진다.\n"
      ],
      "metadata": {
        "id": "sow3OP3Ngm3x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvTOpLsrZ_I8"
      },
      "source": [
        "rdd = spark.sparkContext.parallelize(name_list_json)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIMTLkPdk4ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f14e060-82b8-48f9-9cc6-02370445eaae"
      },
      "source": [
        "rdd"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark이 lazy execution을 하기때문에 이 시점에는 아무것도 하지 않다가 실제 데이터가 계산이 되어야만 가능한 작업들을 하는 순간 그때서야 미뤄놨던 작업들을 실행한다.\n",
        "\n",
        "(spark 클러스터로 올리는 작업을 ..!)"
      ],
      "metadata": {
        "id": "hz1pD01ihuJI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmszVYtzaL3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f02fd11-cf33-4bbe-9176-67f9c89423e3"
      },
      "source": [
        "rdd.count()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD의 장점?! -> lambda function을 쓰기 좋다..?\n",
        "\n",
        "람다 함수를 써서 string을 json형식으로 (dictionary형태로) 바꾸기  \n",
        "\n",
        "** immutable object이기때문에 생성했던 RDD의 값을 바꿀수는 없고 새로운 RDD 생성!"
      ],
      "metadata": {
        "id": "QuLi0zmGiK_u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMLop2SQbMnZ"
      },
      "source": [
        "import json\n",
        "\n",
        "parsed_rdd = rdd.map(lambda el:json.loads(el))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAgNR7EWcl2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82020a56-76cc-49ff-bf4a-da3daa303322"
      },
      "source": [
        "parsed_rdd"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[2] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "collect함수를 이용하여 RDD를 Python쪽으로 가져오기"
      ],
      "metadata": {
        "id": "V5leD5E7jGWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5SMf4IcoIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1844b5e-3458-4909-8973-25ba587d048d"
      },
      "source": [
        "parsed_rdd.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'keeyong'}, {'name': 'benjamin'}, {'name': 'claire'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Kmn_qycqQl"
      },
      "source": [
        "parsed_name_rdd = rdd.map(lambda el:json.loads(el)[\"name\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZWxTKgJnDm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab1ff54-7a10-4586-b9be-a8a45a4a5814"
      },
      "source": [
        "parsed_name_rdd.collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keeyong', 'benjamin', 'claire']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McGKfLtWnsTB"
      },
      "source": [
        "###**파이썬 리스트를 데이터프레임으로 변환하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이썬 리스트나 판다스 데이터프레임이나 RDD를 Spark 데이터 프레임으로 변환하고 싶은 경우에는 Spark 세션에 있는 createDataFrame함수를 이용해주면 된다.\n",
        "\n",
        "createDataFrame함수에서 두 번째 인자로 스키마를 지정해줘야하는데, 파이썬 리스트같은 경우 string 타입이 리스트이므로 일단은 StringType()으로 지정!..\n",
        "이 경우 디폴트로 필드의 이름을 value라고 지정해준다"
      ],
      "metadata": {
        "id": "U6vSBEJqj1kB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WlhiRsKo7j7"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df = spark.createDataFrame(name_list_json, StringType())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2-pyiKGpLZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44450628-ea01-4f22-b7f3-7d441ac63115"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sql4KPhppgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369524c2-8bf5-4deb-d540-1aed61659273"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄴvalue라는 필드가 있고 해당 필드의 데이터 타입이 string이다"
      ],
      "metadata": {
        "id": "GCosc8NOkpk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df의 모든 데이터를 모아서 collect로 파이썬으로 가져오기.."
      ],
      "metadata": {
        "id": "htX7ftUmk6uO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2XzTjdEpuZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb40eadd-4550-4a68-b966-889e6b82ba8e"
      },
      "source": [
        "df.select('*').collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄴRow라는 타입으로 표시되어있는데, 데이터셋에서 데이터프레임을 지원하기 위해 사용되는 타입..\n",
        "\n",
        "일단은 데이터프레임을 파이썬쪽으로 읽어오면 레코드들이 Row타입으로 표시된다고 이해하면 될 것임 ..."
      ],
      "metadata": {
        "id": "JO0LesAIlDm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD를 DataFrame으로 변환해보는 예제: 앞서 parsed_rdd를 DataFrame으로 변환해보자\n",
        "\n"
      ],
      "metadata": {
        "id": "NhmokDHwUBNm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TLUFxgqLrm"
      },
      "source": [
        "df_parsed_rdd = parsed_rdd.toDF()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtFEYRG61-L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87522c6a-4dba-43dd-c970-f01f5b5f162d"
      },
      "source": [
        "df_parsed_rdd.printSchema()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MptOIByKsLeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebba5eb3-50a8-4dbc-fabb-105e6cbd84bf"
      },
      "source": [
        "df_parsed_rdd.select('name').collect()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='keeyong'), Row(name='benjamin'), Row(name='claire')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark 데이터프레임으로 로드해보기"
      ],
      "metadata": {
        "id": "PIDOQxK3Prz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv"
      ],
      "metadata": {
        "id": "PoIJAjvNPYfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65acfca0-d7b5-48e6-f5b3-1fa35606b996"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-19 08:22:21--  https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv\n",
            "Resolving s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)... 52.92.192.18, 52.218.197.89, 52.218.181.49, ...\n",
            "Connecting to s3-geospatial.s3-us-west-2.amazonaws.com (s3-geospatial.s3-us-west-2.amazonaws.com)|52.92.192.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 997 [text/csv]\n",
            "Saving to: ‘name_gender.csv’\n",
            "\n",
            "\rname_gender.csv       0%[                    ]       0  --.-KB/s               \rname_gender.csv     100%[===================>]     997  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-19 08:22:21 (17.1 MB/s) - ‘name_gender.csv’ saved [997/997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"name_gender.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "fvTn2b_dPznk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f270965-2263-4eed-cc2a-3cc2c09ea890"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "header가 있음을 알리고 스키마가 제대로 잡히게 해보자!.."
      ],
      "metadata": {
        "id": "GIB4bcBJmCCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\", True).csv(\"name_gender.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "2GXxjQQTTzyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb88099-b12e-4dca-e790-fb436fc269c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "eyyxGbD1QF9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c58b3a-1e21-4152-e04a-e25de6f951eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|      name|gender|\n",
            "+----------+------+\n",
            "|  Adaleigh|     F|\n",
            "|     Amryn|Unisex|\n",
            "|    Apurva|Unisex|\n",
            "|    Aryion|     M|\n",
            "|    Alixia|     F|\n",
            "|Alyssarose|     F|\n",
            "|    Arvell|     M|\n",
            "|     Aibel|     M|\n",
            "|   Atiyyah|     F|\n",
            "|     Adlie|     F|\n",
            "|    Anyely|     F|\n",
            "|    Aamoni|     F|\n",
            "|     Ahman|     M|\n",
            "|    Arlane|     F|\n",
            "|   Armoney|     F|\n",
            "|   Atzhiry|     F|\n",
            "| Antonette|     F|\n",
            "|   Akeelah|     F|\n",
            "| Abdikadir|     M|\n",
            "|    Arinze|     M|\n",
            "+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "B16yILDkSGbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095b66f6-0114-49c7-bc7b-49c3d87abe26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Adaleigh', gender='F'),\n",
              " Row(name='Amryn', gender='Unisex'),\n",
              " Row(name='Apurva', gender='Unisex'),\n",
              " Row(name='Aryion', gender='M'),\n",
              " Row(name='Alixia', gender='F')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby([\"gender\"]).count().collect()"
      ],
      "metadata": {
        "id": "C60i-1oGSC8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72adda0-5029-4fa2-d081-90d7a61b9e86"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(gender='F', count=65),\n",
              " Row(gender='M', count=28),\n",
              " Row(gender='Unisex', count=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ㄴdf.groupby([\"gender\"]).count()만 쓰면 Spark 클러스터에서만 동작!\n",
        "collect를 호출하여 python 드라이버로 가져와서 내용을 확인하기!"
      ],
      "metadata": {
        "id": "WNyo7a-smUv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "x98icREPokNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0e51d4-5a3b-4b96-c769-32c343083e75"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터프레임을 테이블뷰로 만들어서 SparkSQL로 처리해보기\n",
        "\n",
        "SQL을 활용하면... 굉장히 편리하지요? 간단하고!?"
      ],
      "metadata": {
        "id": "__lQAuE6WQp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"namegender\")"
      ],
      "metadata": {
        "id": "CzaHf3jaWQJM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namegender_group_df = spark.sql(\"SELECT gender, count(1) FROM namegender GROUP BY 1\")"
      ],
      "metadata": {
        "id": "_XZTQIUWWVdU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namegender_group_df.collect()"
      ],
      "metadata": {
        "id": "ungWbu_zWXfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12babf74-4acd-4144-8eb8-a1fdf5eb6291"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(gender='F', count(1)=65),\n",
              " Row(gender='M', count(1)=28),\n",
              " Row(gender='Unisex', count(1)=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어떤 데이터 프레임들이 테이블로 사용가능한지 알고싶으면 -> catalog.listTables"
      ],
      "metadata": {
        "id": "KYH2veE5oU7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "id": "UV3BwA7sJLmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d59705-5466-4ecb-f227-57fac17d4758"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='namegender', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partition의 수 계산해보기"
      ],
      "metadata": {
        "id": "nsBKrZthq6s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "namegender_group_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "ROHFCGikpBQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531d0331-9c23-4019-e668-f58df7d8fb47"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파티션을 두 개로 나누기?"
      ],
      "metadata": {
        "id": "Tcf_-dtsoqj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "two_namegender_group_df = namegender_group_df.repartition(2)"
      ],
      "metadata": {
        "id": "LGyv_owLtTHD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_namegender_group_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "9n6okNwvtaic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee877494-1cb7-4fd7-b36c-1b6cdb2a36c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}